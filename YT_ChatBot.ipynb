{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu faster_whisper yt_dlp google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mPdGFv-wGXtX",
        "outputId": "443c6043-cb36-4b88-9d4a-47c3f99cfb7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting faster_whisper\n",
            "  Downloading faster_whisper-1.2.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2025.10.22-py3-none-any.whl.metadata (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.46.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster_whisper)\n",
            "  Downloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.21 in /usr/local/lib/python3.12/dist-packages (from faster_whisper) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster_whisper) (0.22.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster_whisper)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting av>=11 (from faster_whisper)\n",
            "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster_whisper) (4.67.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster_whisper) (75.2.0)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster_whisper) (6.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (1.2.0)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster_whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (1.13.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster_whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1.14->faster_whisper) (1.3.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faster_whisper-1.2.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.10.22-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt_dlp, humanfriendly, faiss-cpu, ctranslate2, av, coloredlogs, onnxruntime, faster_whisper\n",
            "Successfully installed av-16.0.1 coloredlogs-15.0.1 ctranslate2-4.6.0 faiss-cpu-1.12.0 faster_whisper-1.2.1 humanfriendly-10.0 onnxruntime-1.23.2 yt_dlp-2025.10.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from yt_dlp import YoutubeDL\n",
        "from pydub import AudioSegment\n",
        "from faster_whisper import WhisperModel\n",
        "import torch\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from google import generativeai\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "#  CONFIG\n",
        "genai.configure(api_key=\"AIzaSyAMrngkE6IaNAeojFvbbMOkHLukxgrQ5u8\")\n",
        "whisper = WhisperModel(\"small\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gemini = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "#  SUMMARIZATION FUNCTION\n",
        "def summarize_text(text):\n",
        "    prompt = f\"Provide a detailed, clear summary of the following transcript:\\n\\n{text}\"\n",
        "    response = gemini.generate_content(prompt)\n",
        "    summary = response.text.strip()\n",
        "    return summary\n",
        "\n",
        "\n",
        "#  STEP 1: DOWNLOAD AUDIO\n",
        "def download_audio(youtube_url, output_file=\"./audio/audio_file\"):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': output_file,\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'noplaylist': True,\n",
        "        'quiet': False,\n",
        "        'continuedl': True\n",
        "    }\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(youtube_url, download=True)\n",
        "        duration = info.get(\"duration\", None)\n",
        "        print(f\"Audio downloaded — Duration: {duration//60 if duration else 'Unknown'} minutes\")\n",
        "    return output_file + \".mp3\"\n",
        "\n",
        "\n",
        "#  STEP 2: SPLIT LONG AUDIO\n",
        "def split_audio(file_path, chunk_length_ms=5 * 60 * 1000):  # 5 minutes\n",
        "    print(\"Splitting long audio into chunks...\")\n",
        "    audio = AudioSegment.from_file(file_path)\n",
        "    chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
        "    os.makedirs(\"chunks\", exist_ok=True)\n",
        "    chunk_files = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_path = f\"chunks/chunk_{i}.mp3\"\n",
        "        chunk.export(chunk_path, format=\"mp3\")\n",
        "        chunk_files.append(chunk_path)\n",
        "\n",
        "    print(f\"Split into {len(chunk_files)} chunks.\")\n",
        "    return chunk_files\n",
        "\n",
        "\n",
        "#  STEP 3: TRANSCRIBE AND SUMMARIZE EACH CHUNK\n",
        "def transcribe_and_summarize_chunks(chunk_files):\n",
        "    print(\"\\nTranscribing and summarizing audio chunks...\\n\")\n",
        "    full_transcription = \"\"\n",
        "    summaries = []\n",
        "    text_chunks = []\n",
        "\n",
        "    for i, chunk in enumerate(chunk_files):\n",
        "        print(f\"Processing chunk {i+1}/{len(chunk_files)} ...\")\n",
        "        segments, _ = whisper.transcribe(chunk, beam_size=3)\n",
        "        chunk_text = \" \".join([seg.text for seg in segments]).strip()\n",
        "        text_chunks.append(chunk_text)\n",
        "\n",
        "        print(f\"Chunk {i+1} transcription done. Length: {len(chunk_text)} chars.\")\n",
        "        full_transcription += chunk_text + \" \"\n",
        "\n",
        "        chunk_summary = summarize_text(chunk_text)\n",
        "        summaries.append(f\"--- Summary for chunk {i+1} ---\\n{chunk_summary}\\n\")\n",
        "\n",
        "    full_transcription = full_transcription.strip()\n",
        "    final_summary = \"\\n\".join(summaries).strip()\n",
        "\n",
        "    print(\"\\n=== FINAL CONCATENATED SUMMARY ===\\n\")\n",
        "    print(final_summary)\n",
        "    return full_transcription, final_summary, text_chunks\n",
        "\n",
        "\n",
        "#  STEP 4: BUILD FAISS INDEX\n",
        "def build_faiss_index(text_chunks):\n",
        "    print(\"\\nBuilding FAISS index...\")\n",
        "    embeddings = embedder.encode(text_chunks, convert_to_numpy=True)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    print(f\"Indexed {len(text_chunks)} chunks.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "#  STEP 5: RETRIEVE RELEVANT CONTEXT\n",
        "def retrieve_relevant_chunks(query, index, text_chunks, top_k=3):\n",
        "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_emb, top_k)\n",
        "    return [text_chunks[i] for i in indices[0]]\n",
        "\n",
        "\n",
        "#  STEP 6: GEMINI Q&A\n",
        "def chat_with_gemini(index, text_chunks):\n",
        "    print(\"\\nConversational mode activated! (type 'exit' to quit)\\n\")\n",
        "    conversation_history = []\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Conversation ended.\")\n",
        "            break\n",
        "\n",
        "        # Retrieve relevant context for this turn\n",
        "        context_snippets = retrieve_relevant_chunks(user_input, index, text_chunks)\n",
        "        context = \"\\n\".join(context_snippets)\n",
        "\n",
        "        # Maintain a short conversation history\n",
        "        recent_history = \"\\n\".join(conversation_history[-4:])  # keep last 4 exchanges\n",
        "\n",
        "        # Build prompt\n",
        "        prompt = f\"\"\"You are an educational conversational assistant.\n",
        "        Use the context below from a video transcript and prior chat history to reply naturally.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Chat History:\n",
        "        {recent_history}\n",
        "\n",
        "        User: {user_input}\n",
        "        Assistant:\"\"\"\n",
        "\n",
        "        response = gemini.generate_content(prompt)\n",
        "        reply = response.text.strip()\n",
        "\n",
        "        print(f\"Assistant: {reply}\\n\")\n",
        "\n",
        "        conversation_history.append(f\"User: {user_input}\")\n",
        "        conversation_history.append(f\"Assistant: {reply}\")\n",
        "\n",
        "\n",
        "#  STEP 7: CLEANUP\n",
        "def cleanup_files(audio_path=\"./audio/audio_file.mp3\", chunks_dir=\"chunks\"):\n",
        "    try:\n",
        "        if os.path.exists(audio_path):\n",
        "            os.remove(audio_path)\n",
        "        if os.path.exists(chunks_dir):\n",
        "            for file_name in os.listdir(chunks_dir):\n",
        "                file_path = os.path.join(chunks_dir, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.remove(file_path)\n",
        "        else:\n",
        "            os.makedirs(chunks_dir, exist_ok=True)\n",
        "        print(\"Cleanup complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Cleanup error: {e}\")\n",
        "\n",
        "\n",
        "#  MAIN PIPELINE\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter YouTube URL: \")\n",
        "    audio_path = download_audio(url)\n",
        "    chunk_files = split_audio(audio_path)\n",
        "    transcribed_text, summarized_text, text_chunks = transcribe_and_summarize_chunks(chunk_files)\n",
        "\n",
        "    index = build_faiss_index(text_chunks)\n",
        "\n",
        "    chat_with_gemini(index, text_chunks)\n",
        "\n",
        "    cleanup_files(audio_path, \"chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "q5A8tKG4GLte",
        "outputId": "491f060e-bd3e-4e2d-bd88-8b784f890db6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter YouTube URL: https://www.youtube.com/watch?v=ry9SYnV3svc\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=ry9SYnV3svc\n",
            "[youtube] ry9SYnV3svc: Downloading webpage\n",
            "[youtube] ry9SYnV3svc: Downloading android sdkless player API JSON\n",
            "[youtube] ry9SYnV3svc: Downloading tv client config\n",
            "[youtube] ry9SYnV3svc: Downloading tv player API JSON\n",
            "[youtube] ry9SYnV3svc: Downloading web safari player API JSON\n",
            "[youtube] ry9SYnV3svc: Downloading player e237d4c5-main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] Falling back to generic n function search\n",
            "         player = https://www.youtube.com/s/player/e237d4c5/player_ias.vflset/en_US/base.js\n",
            "WARNING: [youtube] ry9SYnV3svc: nsig extraction failed: Some formats may be missing\n",
            "         n = 5yY62RU_IGIYd_i1IBb ; player = https://www.youtube.com/s/player/e237d4c5/player_ias.vflset/en_US/base.js\n",
            "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
            "WARNING: [youtube] ry9SYnV3svc: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] ry9SYnV3svc: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] ry9SYnV3svc: Downloading m3u8 information\n",
            "[info] ry9SYnV3svc: Downloading 1 format(s): 251-11\n",
            "[download] Destination: ./audio/audio_file\n",
            "[download] 100% of    1.58MiB in 00:00:00 at 10.17MiB/s  \n",
            "[ExtractAudio] Destination: ./audio/audio_file.mp3\n",
            "Deleting original file ./audio/audio_file (pass -k to keep)\n",
            "Audio downloaded — Duration: 2 minutes\n",
            "Splitting long audio into chunks...\n",
            "Split into 1 chunks.\n",
            "\n",
            "Transcribing and summarizing audio chunks...\n",
            "\n",
            "Processing chunk 1/1 ...\n",
            "Chunk 1 transcription done. Length: 1113 chars.\n",
            "\n",
            "=== FINAL CONCATENATED SUMMARY ===\n",
            "\n",
            "--- Summary for chunk 1 ---\n",
            "Mark is discussing his new job and expressing his positive experiences. He enjoys the friendly and helpful coworkers, the energetic and fun atmosphere, and his humorous and flexible boss. He highlights the freedom to set his own hours, arrive and leave work when he chooses, and the relaxed dress code. The other person expresses envy over the casual attire, as they dislike wearing a suit daily. The conversation then shifts to preferred work schedules, with Mark revealing his preference for working early and enjoying the mornings, including running and watching the sunrise. In contrast, the other person identifies as a \"night owl\" who prefers sleeping in and is more alert in the evenings. The conversation ends with a common idiom and the other person considering going to bed earlier. In short, Mark is happy with his new job and they are contrasting their individual preferences for work schedules.\n",
            "\n",
            "Building FAISS index...\n",
            "Indexed 1 chunks.\n",
            "\n",
            "Conversational mode activated! (type 'exit' to quit)\n",
            "\n",
            "You: Was mark loving his job?\n",
            "Assistant: Yes, Mark loves his new job! He says he can't complain because the company is great, his coworkers are friendly, the atmosphere is energetic, and his boss is flexible and hilarious.\n",
            "\n",
            "You: exit\n",
            "Conversation ended.\n",
            "Cleanup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnH6G-kQGTIn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}